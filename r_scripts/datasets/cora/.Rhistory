#Creating the graph object
net <- graph_from_data_frame(d=genre_connections, vertices=genre_weights, directed=F)
# Amount of communities
length(ceb)
#Loading used libraries
library(tidyverse)
library(magrittr)
library(ggplot2)
library(dplyr)
library(hrbrthemes)
library(fmsb)
library(GGally)
#Filter unknown songs
tracks <- tracks %>% filter(name!='')
#Loading CSV
tracks <- read.csv(file = 'D:/Pablo/clases/UJM/2. Semester, 2021/Data Mining/Project/tracks_info.csv')
#duration_ms
tracks %>%
ggplot( aes(x=duration_ms)) +
geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
ggtitle("Duration Ms") +
theme_ipsum()
#danceability
tracks %>%
ggplot( aes(x=danceability)) +
geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
ggtitle("Danceability") +
theme_ipsum()
#popularity
tracks %>%
ggplot( aes(x=popularity)) +
geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
ggtitle("Popularity") +
theme_ipsum()
#energy
tracks %>%
ggplot( aes(x=energy)) +
geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
ggtitle("Energy") +
theme_ipsum()
#key
tracks %>%
ggplot( aes(x=key)) +
geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
ggtitle("Key") +
theme_ipsum()
#loudness
tracks %>%
ggplot( aes(x=loudness)) +
geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
ggtitle("Loudness") +
theme_ipsum()
#mode
tracks %>%
ggplot( aes(x=mode)) +
geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
ggtitle("Mode") +
theme_ipsum()
#speechiness
tracks %>%
ggplot( aes(x=speechiness)) +
geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
ggtitle("Speechiness") +
theme_ipsum()
#instrumentalness
tracks %>%
ggplot( aes(x=instrumentalness)) +
geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
ggtitle("Instrumentalness") +
theme_ipsum()
#acousticness
tracks %>%
ggplot( aes(x=acousticness)) +
geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
ggtitle("Acousticness") +
theme_ipsum()
#valence
tracks %>%
ggplot( aes(x=valence)) +
geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
ggtitle("Valence") +
theme_ipsum()
#liveness
tracks %>%
ggplot( aes(x=liveness)) +
geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
ggtitle("Liveness") +
theme_ipsum()
radar_data <- tracks %>%
select('duration_ms','popularity','danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence') %>%
group_by() %>%
summarize(
duration_ms = mean(duration_ms, na.rm=TRUE),
popularity = mean(popularity/100, na.rm=TRUE),
danceability = mean(danceability, na.rm=TRUE),
energy = mean(energy, na.rm=TRUE),
key = mean(key, na.rm=TRUE),
loudness = mean(loudness, na.rm=TRUE),
mode = mean(mode, na.rm=TRUE),
speechiness = mean(speechiness, na.rm=TRUE),
acousticness = mean(acousticness, na.rm=TRUE),
instrumentalness = mean(instrumentalness, na.rm=TRUE),
liveness = mean(liveness, na.rm=TRUE),
valence = mean(valence, na.rm=TRUE)
)
#Add min
radar_data <- rbind(c(0,0,0,0,0,-60,0,0,0,0,0,0), radar_data)
#Add max
radar_data <- rbind(c(1200000,100,1,1,12,0,1,1,1,1,1,1), radar_data)
#Radar chart
radarchart( radar_data  , axistype=1 ,
#custom polygon
pcol=rgb(0.2,0.5,0.5,0.9) , pfcol=rgb(0.2,0.5,0.5,0.5) , plwd=4 ,
#custom the grid
cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,20,5), cglwd=0.8,
#custom labels
vlcex=0.8
)
#Correlation
ggcorr(tracks, method = c("everything", "pearson"), label = TRUE)
columns <- list('danceability','energy','speechiness','acousticness','instrumentalness','liveness','valence')
View(genre_weights)
View(genre_weights)
View(genre_connections)
View(genre_connections)
#not really a good idea
#Creating the graph object
net <- graph_from_data_frame(d=genre_connections, vertices=genre_weights, directed=F)
# Community detection based on edge betweenness (Newman-Girvan)
ceb <- cluster_edge_betweenness(net, weights = E(graph)$weight)
# Community detection based on edge betweenness (Newman-Girvan)
ceb <- cluster_edge_betweenness(net, weights = E(net)$weight)
# Amount of communities
length(ceb)
Community = induced_subgraph(net, ceb[[i]])
###########  Answer #########
for(i in seq_along(ceb)) {
Community = induced_subgraph(net, ceb[[i]])
V(Community)$name <- ceb[[i]]        ## To preserve original node numbers
EL = as_edgelist(Community)
VL = V(Community)$name
FileName = paste0("D:/Pablo/clases/UJM/2. Semester, 2021/Data Mining/Project/communities/community_", i, ".dat")
write.table(VL, FileName, row.names=FALSE, col.names=FALSE, sep=",")
}
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
group_by(artist,genres) %>% #Reduce to one row per artist, to change how weight behave
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
select(id, name, genres) %>% #Select only the rows we need
rename(genre = genres)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
group_by(artist,genres) %>% #Reduce to one row per artist, to change how weight behave
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
select(id, name, genres) %>% #Select only the rows we need
rename(genre = genres)
View(genre_tracks)
View(genre_tracks)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
group_by(artist,genres) %>% #Reduce to one row per artist, to change how weight behave
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
select(artist, genres) %>% #Select only the rows we need
rename(genre = genres)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
group_by(artist,genres) %>% #Reduce to one row per artist, to change how weight behave
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
select(artist, genres) %>% #Select only the rows we need
rename(genre = genres)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
group_by(artist,genres) %>% #Reduce to one row per artist, to change how weight behave
#separate_rows(genres, sep = ",") %>% #Separate each genre into a row
#select(artist, genres) %>% #Select only the rows we need
rename(genre = genres)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
group_by(artist,genres) %>% #Reduce to one row per artist, to change how weight behave
#separate_rows(genres, sep = ",") %>% #Separate each genre into a row
select(artist, genres) %>% #Select only the rows we need
rename(genre = genres)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
select(artist, genres) %>% #Select only the rows we need
unique() %>% # Remove weight by amount of songs by each artist
#separate_rows(genres, sep = ",") %>% #Separate each genre into a row
rename(genre = genres)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
select(artist, genres) %>% #Select only the rows we need
unique() %>% # Remove weight by amount of songs by each artist
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
rename(genre = genres)
#Group by genre, count songs with specific genre
genre_weights <- genre_tracks %>%
group_by(genre) %>%
count() %>%
rename(weight = n)
#The connections in the graph
genre_connections <-
#inner_join(genre_tracks, genre_tracks, by=c("id" = "id"), suffix = c(".x", ".y")) %>% #Inner joinning with the same song
inner_join(genre_tracks, genre_tracks, by=c("artist" = "artist"), suffix = c(".x", ".y")) %>% #Inner joinning with the same artist
filter(genre.x != genre.y) %>% # Remove connections with itself
select(genre.x, genre.y) %>% # Select only the pair of genres
filter(genre.x < genre.y) %>% # To avoid duplicates, keep only ordered genres
group_by(genre.x, genre.y) %>% # Group by genre pairs
count() %>%
rename(weight = n)
#not really a good idea
#Creating the graph object
net <- graph_from_data_frame(d=genre_connections, vertices=genre_weights, directed=F)
# Community detection based on edge betweenness (Newman-Girvan)
ceb <- cluster_edge_betweenness(net, weights = E(net)$weight)
# Amount of communities
length(ceb)
###########  Answer #########
for(i in seq_along(ceb)) {
Community = induced_subgraph(net, ceb[[i]])
V(Community)$name <- ceb[[i]]        ## To preserve original node numbers
EL = as_edgelist(Community)
VL = V(Community)$name
FileName = paste0("D:/Pablo/clases/UJM/2. Semester, 2021/Data Mining/Project/communities/community_", i, ".dat")
write.table(VL, FileName, row.names=FALSE, col.names=FALSE, sep=",")
}
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
select(artist, genres) %>% #Select only the rows we need
group_by(artist, genres) %>% #Grouping to keep uniques
count() %>% #Count
unique() %>% # Remove weight by amount of songs by each artist
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
rename(genre = genres)
View(genre_tracks)
View(genre_tracks)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
select(artist, genres) %>% #Select only the rows we need
group_by(artist, genres) %>% #Grouping to keep uniques
count() %>% #Count
#unique() %>% # Remove weight by amount of songs by each artist
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
rename(genre = genres)
rm(list = ls())
# base path
datsets_location <- 'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Graphs Visualization/datasets/'
# Loading data
wikipedia_nodes <- read.csv(file=paste(datsets_location,'wikipedia/Wiki-Vote.txt',sep = ''), sep = '\t')
net <- graph_from_data_frame(d=wikipedia_nodes, directed=FALSE)
# Loading used libraries
library(tidyverse)
library(igraph)
library(GGally)
library(stats)
#Creating the graph object
net <- graph_from_data_frame(d=wikipedia_nodes, directed=FALSE)
# Calculationg
num_vertex <- length(V(net))
num_edges <- length(E(net))
#https://igraph.org/r/doc/diameter.html
net_density <- graph.density(net)
#Degree distribution
vertex_degrees <- as.data.frame(table(degree(net))) %>% rename(degree = Var1)
#https://igraph.org/r/doc/diameter.html
net_diameter <- diameter(net)
#TODO Ask does it really requires memberships?
# if not, how to work without memebership
#https://igraph.org/r/doc/modularity.igraph.html
net_modularity <- modularity(net)
clustering_coefficient <- transitivity(net, type = 'average')
# What to do with this?
Connected_components <- components(net)
#https://igraph.org/r/doc/betweenness.html
net_betweenness <- mean(betweenness(net))
#https://igraph.org/r/doc/closeness.html
net_closeness <- mean(closeness(net))
#Plotting everything!
# Plotting Degrees
vertex_degrees %>%
ggplot( aes(x=degree, y=Freq)) +
geom_line(group=1, color="black") +
geom_point(shape=21, color="black", fill="#777777", size=5) +
annotate("text",
x = max(as.numeric(vertex_degrees$degree))-max(as.numeric(vertex_degrees$degree))/3,
y = max(vertex_degrees$Freq)-(max(vertex_degrees$Freq)/8),
hjust = 0,
label = paste(paste('Nodes:',format(num_vertex,  big.mark = ',')),
paste('Edges:',format(num_edges,  big.mark = ',')),
paste('Diameter:',net_diameter),
paste('Density:',format(net_density, nsmall = 6)),
paste('Clustering Coeff:',format(clustering_coefficient, nsmall = 4)),
paste('Betweeness:',format(net_betweenness, nsmall = 2)),
paste('Closeness:',format(net_closeness, nsmall = 2)),
sep='\n')) +
ggtitle("Facebook users")
#Plotting everything!
# Plotting Degrees
vertex_degrees %>%
ggplot( aes(x=degree, y=Freq)) +
geom_line(group=1, color="black") +
geom_point(shape=21, color="black", fill="#777777", size=5) +
annotate("text",
x = max(as.numeric(vertex_degrees$degree))-max(as.numeric(vertex_degrees$degree))/3,
y = max(vertex_degrees$Freq)-(max(vertex_degrees$Freq)/8),
hjust = 0,
label = paste(paste('Nodes:',format(num_vertex,  big.mark = ',')),
paste('Edges:',format(num_edges,  big.mark = ',')),
paste('Diameter:',net_diameter),
paste('Density:',format(net_density, nsmall = 6)),
paste('Clustering Coeff:',format(clustering_coefficient, nsmall = 4)),
paste('Betweeness:',format(net_betweenness, nsmall = 2)),
paste('Closeness:',format(net_closeness, nsmall = 2)),
sep='\n')) +
ggtitle("Wikipedia votes users")
#Plotting everything!
# Plotting Degrees
vertex_degrees %>%
ggplot( aes(x=degree, y=Freq)) +
geom_line(group=1, color="black") +
geom_point(shape=21, color="black", fill="#777777", size=5) +
annotate("text",
x = max(as.numeric(vertex_degrees$degree))-max(as.numeric(vertex_degrees$degree))/3,
y = max(vertex_degrees$Freq)-(max(vertex_degrees$Freq)/8),
hjust = 0,
label = paste(paste('Nodes:',format(num_vertex,  big.mark = ',')),
paste('Edges:',format(num_edges,  big.mark = ',')),
paste('Diameter:',net_diameter),
paste('Density:',format(net_density, nsmall = 6)),
paste('Clustering Coeff:',format(clustering_coefficient, nsmall = 4)),
paste('Betweeness:',format(net_betweenness, nsmall = 2)),
paste('Closeness:',format(net_closeness, nsmall = 2)),
sep='\n')) +
ggtitle("Wikipedia votes")
#Save graph
#https://igraph.org/r/doc/write_graph.html
write_graph(net, paste(datsets_location,'wikipedia.txt',sep=''), format='pajek')
clustering_coefficient <- transitivity(net, type = 'global') # More weight on high degree nodes
clustering_coefficient <- mean(transitivity(net, type = 'local'), na.rm=TRUE) # Same weight on all
clustering_coefficient <- transitivity(net, type = 'average')
# base path
datsets_location <- 'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Graphs Visualization/datasets/'
# Loading data
deezer_nodes <- read.csv(file=paste(datsets_location,'deezer/deezer_europe_edges.csv',sep = ''), sep = ',')
#Creating the graph object
net <- graph_from_data_frame(d=wikipedia_nodes, directed=FALSE)
#Creating the graph object
net <- graph_from_data_frame(d=deezer_nodes, directed=FALSE)
# Calculationg
num_vertex <- length(V(net))
num_edges <- length(E(net))
#https://igraph.org/r/doc/diameter.html
net_density <- graph.density(net)
#Degree distribution
vertex_degrees <- as.data.frame(table(degree(net))) %>% rename(degree = Var1)
#https://igraph.org/r/doc/diameter.html
net_diameter <- diameter(net)
clustering_coefficient <- transitivity(net, type = 'average')
# What to do with this?
Connected_components <- components(net)
#https://igraph.org/r/doc/betweenness.html
net_betweenness <- mean(betweenness(net))
#https://igraph.org/r/doc/closeness.html
net_closeness <- mean(closeness(net))
#Plotting everything!
# Plotting Degrees
vertex_degrees %>%
ggplot( aes(x=degree, y=Freq)) +
geom_line(group=1, color="black") +
geom_point(shape=21, color="black", fill="#ff0092", size=5) +
annotate("text",
x = max(as.numeric(vertex_degrees$degree))-max(as.numeric(vertex_degrees$degree))/3,
y = max(vertex_degrees$Freq)-(max(vertex_degrees$Freq)/8),
hjust = 0,
label = paste(paste('Nodes:',format(num_vertex,  big.mark = ',')),
paste('Edges:',format(num_edges,  big.mark = ',')),
paste('Diameter:',net_diameter),
paste('Density:',format(net_density, nsmall = 6)),
paste('Clustering Coeff:',format(clustering_coefficient, nsmall = 4)),
paste('Betweeness:',format(net_betweenness, nsmall = 2)),
paste('Closeness:',format(net_closeness, nsmall = 2)),
sep='\n')) +
ggtitle("Deezer Users")
# base path
datsets_location <- 'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Graphs Visualization/datasets/'
# Loading data
lastfm_nodes <- read.csv(file=paste(datsets_location,'lastfm/lastfm_asia_edges.csv',sep = ''), sep = ',')
#Creating the graph object
net <- graph_from_data_frame(d=lastfm_nodes, directed=FALSE)
# Calculationg
num_vertex <- length(V(net))
num_edges <- length(E(net))
#https://igraph.org/r/doc/diameter.html
net_density <- graph.density(net)
#Degree distribution
vertex_degrees <- as.data.frame(table(degree(net))) %>% rename(degree = Var1)
#https://igraph.org/r/doc/diameter.html
net_diameter <- diameter(net)
clustering_coefficient <- transitivity(net, type = 'average')
# What to do with this?
Connected_components <- components(net)
#https://igraph.org/r/doc/betweenness.html
net_betweenness <- mean(betweenness(net))
#https://igraph.org/r/doc/closeness.html
net_closeness <- mean(closeness(net))
#Plotting everything!
# Plotting Degrees
vertex_degrees %>%
ggplot( aes(x=degree, y=Freq)) +
geom_line(group=1, color="black") +
geom_point(shape=21, color="black", fill="#d51007", size=5) +
annotate("text",
x = max(as.numeric(vertex_degrees$degree))-max(as.numeric(vertex_degrees$degree))/3,
y = max(vertex_degrees$Freq)-(max(vertex_degrees$Freq)/8),
hjust = 0,
label = paste(paste('Nodes:',format(num_vertex,  big.mark = ',')),
paste('Edges:',format(num_edges,  big.mark = ',')),
paste('Diameter:',net_diameter),
paste('Density:',format(net_density, nsmall = 6)),
paste('Clustering Coeff:',format(clustering_coefficient, nsmall = 4)),
paste('Betweeness:',format(net_betweenness, nsmall = 2)),
paste('Closeness:',format(net_closeness, nsmall = 2)),
sep='\n')) +
ggtitle("LastFM Users")
#Save graph
#https://igraph.org/r/doc/write_graph.html
write_graph(net, paste(datsets_location,'lastfm_users.txt',sep=''), format='pajek')
# base path
datsets_location <- 'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Graphs Visualization/datasets/'
# Loading data
deezer_nodes <- read.csv(file=paste(datsets_location,'deezer/deezer_europe_edges.csv',sep = ''), sep = ',')
#Creating the graph object
net <- graph_from_data_frame(d=deezer_nodes, directed=FALSE)
#Save graph
#https://igraph.org/r/doc/write_graph.html
write_graph(net, paste(datsets_location,'deezer_users.txt',sep=''), format='pajek')
# Loading used libraries
library(tidyverse)
library(igraph)
library(GGally)
library(stats)
# base path
datsets_location <- 'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Graphs Visualization/datasets/'
# Loading data
twitter_nodes <- read.csv(file=paste(datsets_location,'twitter/twitter_combined.txt',sep = ''), sep = ' ', header=FALSE)
#Creating the graph object
net <- graph_from_data_frame(d=twitter_nodes, directed=FALSE)
# Clean working space
rm(list = ls())
# Loading used libraries
library(tidyverse)
library(igraph)
library(GGally)
library(stats)
# base path
datsets_location <- 'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Graphs Visualization/datasets/'
setwd(paste(datsets_location,'cora/',sep = ''))
dataset_name <- 'Cora'
dataset_color <- '#21cbc'
# Loading data
cora_cites <- read.csv(file='cora.cites', sep = '\t', header=FALSE)
cora_content <- read.csv(file='cora.content', sep = '\t', header=FALSE)
labels_num$numeric <- 1:nrow(labels_num)
# Numerical values to be given to each class
labels_num <- cora_content %>%
select(V1435) %>%
rename(label=V1435) %>%
unique()
cora_labels <- cora_content %>%
select(V1,V1435) %>%
rename(paper=V1,label=V1435) %>%
left_join(labels_num, by=c('label'='label'), suffix = c(".x", ".y"))
write.csv(cora_cites,'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Repository/datasets/cora/cities.csv', row.names = TRUE)
write.csv(cora_labels,'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Repository/datasets/cora/labels.csv', row.names = TRUE)
write.csv(cora_cites,'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Repository/datasets/cora/cities.csv', row.names = FALSE)
write.csv(cora_labels,'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Repository/datasets/cora/labels.csv', row.names = FALSE)
cora_labels <- cora_content %>%
select(V1,V1435) %>%
rename(paper=V1,label=V1435) %>%
left_join(labels_num, by=c('label'='label'), suffix = c(".x", ".y"))
# Numerical values to be given to each class
labels_num <- cora_content %>%
select(V1435) %>%
rename(label=V1435) %>%
unique()
labels_num$numeric <- 1:nrow(labels_num)
cora_labels <- cora_content %>%
select(V1,V1435) %>%
rename(paper=V1,label=V1435) %>%
left_join(labels_num, by=c('label'='label'), suffix = c(".x", ".y"))
# Saving the source info into csvs
write.csv(cora_cites,'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Repository/datasets/cora/cities.csv', row.names = FALSE)
write.csv(cora_labels,'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Repository/datasets/cora/labels.csv', row.names = FALSE)
