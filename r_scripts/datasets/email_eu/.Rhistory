#not really a good idea
#Creating the graph object
net <- graph_from_data_frame(d=genre_connections, vertices=genre_weights, directed=F)
# Community detection based on edge betweenness (Newman-Girvan)
ceb <- cluster_edge_betweenness(net, weights = E(graph)$weight)
# Community detection based on edge betweenness (Newman-Girvan)
ceb <- cluster_edge_betweenness(net, weights = E(net)$weight)
# Amount of communities
length(ceb)
Community = induced_subgraph(net, ceb[[i]])
###########  Answer #########
for(i in seq_along(ceb)) {
Community = induced_subgraph(net, ceb[[i]])
V(Community)$name <- ceb[[i]]        ## To preserve original node numbers
EL = as_edgelist(Community)
VL = V(Community)$name
FileName = paste0("D:/Pablo/clases/UJM/2. Semester, 2021/Data Mining/Project/communities/community_", i, ".dat")
write.table(VL, FileName, row.names=FALSE, col.names=FALSE, sep=",")
}
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
group_by(artist,genres) %>% #Reduce to one row per artist, to change how weight behave
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
select(id, name, genres) %>% #Select only the rows we need
rename(genre = genres)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
group_by(artist,genres) %>% #Reduce to one row per artist, to change how weight behave
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
select(id, name, genres) %>% #Select only the rows we need
rename(genre = genres)
View(genre_tracks)
View(genre_tracks)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
group_by(artist,genres) %>% #Reduce to one row per artist, to change how weight behave
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
select(artist, genres) %>% #Select only the rows we need
rename(genre = genres)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
group_by(artist,genres) %>% #Reduce to one row per artist, to change how weight behave
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
select(artist, genres) %>% #Select only the rows we need
rename(genre = genres)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
group_by(artist,genres) %>% #Reduce to one row per artist, to change how weight behave
#separate_rows(genres, sep = ",") %>% #Separate each genre into a row
#select(artist, genres) %>% #Select only the rows we need
rename(genre = genres)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
group_by(artist,genres) %>% #Reduce to one row per artist, to change how weight behave
#separate_rows(genres, sep = ",") %>% #Separate each genre into a row
select(artist, genres) %>% #Select only the rows we need
rename(genre = genres)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
select(artist, genres) %>% #Select only the rows we need
unique() %>% # Remove weight by amount of songs by each artist
#separate_rows(genres, sep = ",") %>% #Separate each genre into a row
rename(genre = genres)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
select(artist, genres) %>% #Select only the rows we need
unique() %>% # Remove weight by amount of songs by each artist
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
rename(genre = genres)
#Group by genre, count songs with specific genre
genre_weights <- genre_tracks %>%
group_by(genre) %>%
count() %>%
rename(weight = n)
#The connections in the graph
genre_connections <-
#inner_join(genre_tracks, genre_tracks, by=c("id" = "id"), suffix = c(".x", ".y")) %>% #Inner joinning with the same song
inner_join(genre_tracks, genre_tracks, by=c("artist" = "artist"), suffix = c(".x", ".y")) %>% #Inner joinning with the same artist
filter(genre.x != genre.y) %>% # Remove connections with itself
select(genre.x, genre.y) %>% # Select only the pair of genres
filter(genre.x < genre.y) %>% # To avoid duplicates, keep only ordered genres
group_by(genre.x, genre.y) %>% # Group by genre pairs
count() %>%
rename(weight = n)
#not really a good idea
#Creating the graph object
net <- graph_from_data_frame(d=genre_connections, vertices=genre_weights, directed=F)
# Community detection based on edge betweenness (Newman-Girvan)
ceb <- cluster_edge_betweenness(net, weights = E(net)$weight)
# Amount of communities
length(ceb)
###########  Answer #########
for(i in seq_along(ceb)) {
Community = induced_subgraph(net, ceb[[i]])
V(Community)$name <- ceb[[i]]        ## To preserve original node numbers
EL = as_edgelist(Community)
VL = V(Community)$name
FileName = paste0("D:/Pablo/clases/UJM/2. Semester, 2021/Data Mining/Project/communities/community_", i, ".dat")
write.table(VL, FileName, row.names=FALSE, col.names=FALSE, sep=",")
}
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
select(artist, genres) %>% #Select only the rows we need
group_by(artist, genres) %>% #Grouping to keep uniques
count() %>% #Count
unique() %>% # Remove weight by amount of songs by each artist
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
rename(genre = genres)
View(genre_tracks)
View(genre_tracks)
#Get track/genre tuple
genre_tracks <- tracks %>%
filter(name!='') %>%  #Filter unknown songs
filter(genres!='') %>% #Filter songs without genres
select(artist, genres) %>% #Select only the rows we need
group_by(artist, genres) %>% #Grouping to keep uniques
count() %>% #Count
#unique() %>% # Remove weight by amount of songs by each artist
separate_rows(genres, sep = ",") %>% #Separate each genre into a row
rename(genre = genres)
rm(list = ls())
# base path
datsets_location <- 'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Graphs Visualization/datasets/'
# Loading data
wikipedia_nodes <- read.csv(file=paste(datsets_location,'wikipedia/Wiki-Vote.txt',sep = ''), sep = '\t')
net <- graph_from_data_frame(d=wikipedia_nodes, directed=FALSE)
# Loading used libraries
library(tidyverse)
library(igraph)
library(GGally)
library(stats)
#Creating the graph object
net <- graph_from_data_frame(d=wikipedia_nodes, directed=FALSE)
# Calculationg
num_vertex <- length(V(net))
num_edges <- length(E(net))
#https://igraph.org/r/doc/diameter.html
net_density <- graph.density(net)
#Degree distribution
vertex_degrees <- as.data.frame(table(degree(net))) %>% rename(degree = Var1)
#https://igraph.org/r/doc/diameter.html
net_diameter <- diameter(net)
#TODO Ask does it really requires memberships?
# if not, how to work without memebership
#https://igraph.org/r/doc/modularity.igraph.html
net_modularity <- modularity(net)
clustering_coefficient <- transitivity(net, type = 'average')
# What to do with this?
Connected_components <- components(net)
#https://igraph.org/r/doc/betweenness.html
net_betweenness <- mean(betweenness(net))
#https://igraph.org/r/doc/closeness.html
net_closeness <- mean(closeness(net))
#Plotting everything!
# Plotting Degrees
vertex_degrees %>%
ggplot( aes(x=degree, y=Freq)) +
geom_line(group=1, color="black") +
geom_point(shape=21, color="black", fill="#777777", size=5) +
annotate("text",
x = max(as.numeric(vertex_degrees$degree))-max(as.numeric(vertex_degrees$degree))/3,
y = max(vertex_degrees$Freq)-(max(vertex_degrees$Freq)/8),
hjust = 0,
label = paste(paste('Nodes:',format(num_vertex,  big.mark = ',')),
paste('Edges:',format(num_edges,  big.mark = ',')),
paste('Diameter:',net_diameter),
paste('Density:',format(net_density, nsmall = 6)),
paste('Clustering Coeff:',format(clustering_coefficient, nsmall = 4)),
paste('Betweeness:',format(net_betweenness, nsmall = 2)),
paste('Closeness:',format(net_closeness, nsmall = 2)),
sep='\n')) +
ggtitle("Facebook users")
#Plotting everything!
# Plotting Degrees
vertex_degrees %>%
ggplot( aes(x=degree, y=Freq)) +
geom_line(group=1, color="black") +
geom_point(shape=21, color="black", fill="#777777", size=5) +
annotate("text",
x = max(as.numeric(vertex_degrees$degree))-max(as.numeric(vertex_degrees$degree))/3,
y = max(vertex_degrees$Freq)-(max(vertex_degrees$Freq)/8),
hjust = 0,
label = paste(paste('Nodes:',format(num_vertex,  big.mark = ',')),
paste('Edges:',format(num_edges,  big.mark = ',')),
paste('Diameter:',net_diameter),
paste('Density:',format(net_density, nsmall = 6)),
paste('Clustering Coeff:',format(clustering_coefficient, nsmall = 4)),
paste('Betweeness:',format(net_betweenness, nsmall = 2)),
paste('Closeness:',format(net_closeness, nsmall = 2)),
sep='\n')) +
ggtitle("Wikipedia votes users")
#Plotting everything!
# Plotting Degrees
vertex_degrees %>%
ggplot( aes(x=degree, y=Freq)) +
geom_line(group=1, color="black") +
geom_point(shape=21, color="black", fill="#777777", size=5) +
annotate("text",
x = max(as.numeric(vertex_degrees$degree))-max(as.numeric(vertex_degrees$degree))/3,
y = max(vertex_degrees$Freq)-(max(vertex_degrees$Freq)/8),
hjust = 0,
label = paste(paste('Nodes:',format(num_vertex,  big.mark = ',')),
paste('Edges:',format(num_edges,  big.mark = ',')),
paste('Diameter:',net_diameter),
paste('Density:',format(net_density, nsmall = 6)),
paste('Clustering Coeff:',format(clustering_coefficient, nsmall = 4)),
paste('Betweeness:',format(net_betweenness, nsmall = 2)),
paste('Closeness:',format(net_closeness, nsmall = 2)),
sep='\n')) +
ggtitle("Wikipedia votes")
#Save graph
#https://igraph.org/r/doc/write_graph.html
write_graph(net, paste(datsets_location,'wikipedia.txt',sep=''), format='pajek')
clustering_coefficient <- transitivity(net, type = 'global') # More weight on high degree nodes
clustering_coefficient <- mean(transitivity(net, type = 'local'), na.rm=TRUE) # Same weight on all
clustering_coefficient <- transitivity(net, type = 'average')
# base path
datsets_location <- 'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Graphs Visualization/datasets/'
# Loading data
deezer_nodes <- read.csv(file=paste(datsets_location,'deezer/deezer_europe_edges.csv',sep = ''), sep = ',')
#Creating the graph object
net <- graph_from_data_frame(d=wikipedia_nodes, directed=FALSE)
#Creating the graph object
net <- graph_from_data_frame(d=deezer_nodes, directed=FALSE)
# Calculationg
num_vertex <- length(V(net))
num_edges <- length(E(net))
#https://igraph.org/r/doc/diameter.html
net_density <- graph.density(net)
#Degree distribution
vertex_degrees <- as.data.frame(table(degree(net))) %>% rename(degree = Var1)
#https://igraph.org/r/doc/diameter.html
net_diameter <- diameter(net)
clustering_coefficient <- transitivity(net, type = 'average')
# What to do with this?
Connected_components <- components(net)
#https://igraph.org/r/doc/betweenness.html
net_betweenness <- mean(betweenness(net))
#https://igraph.org/r/doc/closeness.html
net_closeness <- mean(closeness(net))
#Plotting everything!
# Plotting Degrees
vertex_degrees %>%
ggplot( aes(x=degree, y=Freq)) +
geom_line(group=1, color="black") +
geom_point(shape=21, color="black", fill="#ff0092", size=5) +
annotate("text",
x = max(as.numeric(vertex_degrees$degree))-max(as.numeric(vertex_degrees$degree))/3,
y = max(vertex_degrees$Freq)-(max(vertex_degrees$Freq)/8),
hjust = 0,
label = paste(paste('Nodes:',format(num_vertex,  big.mark = ',')),
paste('Edges:',format(num_edges,  big.mark = ',')),
paste('Diameter:',net_diameter),
paste('Density:',format(net_density, nsmall = 6)),
paste('Clustering Coeff:',format(clustering_coefficient, nsmall = 4)),
paste('Betweeness:',format(net_betweenness, nsmall = 2)),
paste('Closeness:',format(net_closeness, nsmall = 2)),
sep='\n')) +
ggtitle("Deezer Users")
# base path
datsets_location <- 'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Graphs Visualization/datasets/'
# Loading data
lastfm_nodes <- read.csv(file=paste(datsets_location,'lastfm/lastfm_asia_edges.csv',sep = ''), sep = ',')
#Creating the graph object
net <- graph_from_data_frame(d=lastfm_nodes, directed=FALSE)
# Calculationg
num_vertex <- length(V(net))
num_edges <- length(E(net))
#https://igraph.org/r/doc/diameter.html
net_density <- graph.density(net)
#Degree distribution
vertex_degrees <- as.data.frame(table(degree(net))) %>% rename(degree = Var1)
#https://igraph.org/r/doc/diameter.html
net_diameter <- diameter(net)
clustering_coefficient <- transitivity(net, type = 'average')
# What to do with this?
Connected_components <- components(net)
#https://igraph.org/r/doc/betweenness.html
net_betweenness <- mean(betweenness(net))
#https://igraph.org/r/doc/closeness.html
net_closeness <- mean(closeness(net))
#Plotting everything!
# Plotting Degrees
vertex_degrees %>%
ggplot( aes(x=degree, y=Freq)) +
geom_line(group=1, color="black") +
geom_point(shape=21, color="black", fill="#d51007", size=5) +
annotate("text",
x = max(as.numeric(vertex_degrees$degree))-max(as.numeric(vertex_degrees$degree))/3,
y = max(vertex_degrees$Freq)-(max(vertex_degrees$Freq)/8),
hjust = 0,
label = paste(paste('Nodes:',format(num_vertex,  big.mark = ',')),
paste('Edges:',format(num_edges,  big.mark = ',')),
paste('Diameter:',net_diameter),
paste('Density:',format(net_density, nsmall = 6)),
paste('Clustering Coeff:',format(clustering_coefficient, nsmall = 4)),
paste('Betweeness:',format(net_betweenness, nsmall = 2)),
paste('Closeness:',format(net_closeness, nsmall = 2)),
sep='\n')) +
ggtitle("LastFM Users")
#Save graph
#https://igraph.org/r/doc/write_graph.html
write_graph(net, paste(datsets_location,'lastfm_users.txt',sep=''), format='pajek')
# base path
datsets_location <- 'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Graphs Visualization/datasets/'
# Loading data
deezer_nodes <- read.csv(file=paste(datsets_location,'deezer/deezer_europe_edges.csv',sep = ''), sep = ',')
#Creating the graph object
net <- graph_from_data_frame(d=deezer_nodes, directed=FALSE)
#Save graph
#https://igraph.org/r/doc/write_graph.html
write_graph(net, paste(datsets_location,'deezer_users.txt',sep=''), format='pajek')
# Loading used libraries
library(tidyverse)
library(igraph)
library(GGally)
library(stats)
# base path
datsets_location <- 'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Graphs Visualization/datasets/'
# Loading data
twitter_nodes <- read.csv(file=paste(datsets_location,'twitter/twitter_combined.txt',sep = ''), sep = ' ', header=FALSE)
#Creating the graph object
net <- graph_from_data_frame(d=twitter_nodes, directed=FALSE)
# Clean working space
rm(list = ls())
# Loading used libraries
library(tidyverse)
library(igraph)
library(GGally)
library(stats)
# base path
datsets_location <- 'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Repository/r_scripts/datasets/email_eu'
setwd(paste(datsets_location,'cora/',sep = ''))
# base path
datsets_location <- 'D:/Pablo/clases/UJM/2. Semester, 2021/Mining Uncertain Social Networks/Repository/r_scripts/datasets/'
setwd(paste(datsets_location,'email_eu/',sep = ''))
dataset_name <- 'email_eu'
dataset_color <- '#21cbc'
# Loading data
cora_cites <- read.csv(file='cora.cites', sep = '\t', header=FALSE)
# Loading data
email_edges <- read.csv(file='email-Eu-core.txt', sep = ' ', header=FALSE)
email_labels <- read.csv(file='email-Eu-core-department-labels.txt', sep = ' ', header=FALSE)
# Creating the graph object
net <- graph_from_data_frame(d=email_edges, vertices=email_labels, directed=FALSE)
# To avoid multiplex graphs
net <- simplify(net)
# Calculationg
num_vertex <- length(V(net))
num_edges <- length(E(net))
#https://igraph.org/r/doc/diameter.html
net_density <- graph.density(net)
#Degree distribution
vertex_degrees <- as.data.frame(table(degree(net))) %>% rename(degree = Var1)
#https://igraph.org/r/doc/diameter.html
net_diameter <- diameter(net)
#https://igraph.org/r/doc/modularity.igraph.html
net_modularity <- modularity(net, as_data_frame(net, what='vertices')$numeric)
#https://igraph.org/r/doc/transitivity.html
#https://stackoverflow.com/questions/48853610/average-clustering-coefficient-of-a-network-igraph
clustering_coefficient <- transitivity(net, type = 'average')
# What to do with this?
Connected_components <- components(net)
#https://igraph.org/r/doc/betweenness.html
net_betweenness <- mean(betweenness(net))
#https://igraph.org/r/doc/modularity.igraph.html
net_modularity <- modularity(net, as_data_frame(net, what='vertices')$V2)
as_data_frame(net, what='vertices')$V2
len(as_data_frame(net, what='vertices')$V2)
length(as_data_frame(net, what='vertices')$V2)
#https://igraph.org/r/doc/modularity.igraph.html
net_modularity <- modularity(net, as_data_frame(net, what='vertices')$V2)
as_data_frame(net, what='vertices')$V2
email_labels$V2 <- email_labels$V2 +1
# Creating the graph object
net <- graph_from_data_frame(d=email_edges, vertices=email_labels, directed=FALSE)
# To avoid multiplex graphs
net <- simplify(net)
# Calculationg
num_vertex <- length(V(net))
num_edges <- length(E(net))
#https://igraph.org/r/doc/diameter.html
net_density <- graph.density(net)
#Degree distribution
vertex_degrees <- as.data.frame(table(degree(net))) %>% rename(degree = Var1)
#https://igraph.org/r/doc/diameter.html
net_diameter <- diameter(net)
#https://igraph.org/r/doc/modularity.igraph.html
net_modularity <- modularity(net, as_data_frame(net, what='vertices')$V2)
#https://igraph.org/r/doc/transitivity.html
#https://stackoverflow.com/questions/48853610/average-clustering-coefficient-of-a-network-igraph
clustering_coefficient <- transitivity(net, type = 'average')
# What to do with this?
Connected_components <- components(net)
#https://igraph.org/r/doc/betweenness.html
net_betweenness <- mean(betweenness(net))
# Vertex betweenness
vertext_betweennes <- as.data.frame(table(betweenness(net))) %>%
rename(betweenness = Var1)
vertext_betweennes$betweenness <- as.numeric(vertext_betweennes$betweenness)
vertext_betweennes$betweenness <- (vertext_betweennes$betweenness - min(vertext_betweennes$betweenness))/(max(vertext_betweennes$betweenness)-min(vertext_betweennes$betweenness))
#https://igraph.org/r/doc/closeness.html
net_closeness <- mean(closeness(net))
#https://igraph.org/r/doc/components.html
connected_components <- count_components(net)
# Visualization
# Source: https://briatte.github.io/ggnet/
ggnet2(net, node.size = 2, color = 'label', edge.size = 1, edge.color = 'grey', palette = 'Set1')
#Plotting everything!
# Plotting Degrees
vertex_degrees %>%
ggplot( aes(x=degree, y=Freq)) +
geom_line(group=1, color="black") +
geom_point(shape=21, color="black", fill="#1DA1F2", size=5) +
annotate("text",
x = max(as.numeric(vertex_degrees$degree))-max(as.numeric(vertex_degrees$degree))/3,
y = max(vertex_degrees$Freq)-(max(vertex_degrees$Freq)/6),
hjust = 0,
label = paste(paste('Nodes:',format(num_vertex,  big.mark = ',')),
paste('Edges:',format(num_edges,  big.mark = ',')),
paste('Diameter:',net_diameter),
paste('Density:',format(net_density, nsmall = 6)),
paste('Modularity:',format(net_modularity, nsmall = 2)),
paste('Clustering Coeff:',format(clustering_coefficient, nsmall = 4)),
paste('Betweeness:',format(net_betweenness, nsmall = 2)),
paste('Closeness:',format(net_closeness, nsmall = 2)),
paste('Connected Components:',connected_components),
sep='\n')) +
ggtitle(paste(dataset_name,"Graph"))
#Read graph
#https://igraph.org/r/doc/read_graph.html
net <- read_graph('cora_graph.txt', format='pajek')
datasets_summary <- datasets_summary %>% filter(Name!=dataset_name)
#Plotting betweenness
# Plotting Degrees
vertext_betweennes %>%
mutate(betweenness=round(betweenness, 1)) %>%
group_by(betweenness) %>%
summarise(Freq=sum(Freq)) %>%
ggplot( aes(x=betweenness, y=Freq)) +
geom_line(group=1, color="black") +
geom_point(shape=21, color="black", fill="#1DA1F2", size=5) +
ggtitle(paste(dataset_name,"Betweenness"))
#Save graph
#https://igraph.org/r/doc/write_graph.html
write_graph(net, 'cora_graph.txt', format='pajek')
# Save a general summary of all
datasets_summary <- read.csv(file=paste(datsets_location,'datasets_summary.csv',sep=''), sep = ',')
datasets_summary <- datasets_summary %>% add_row(
Name=dataset_name,
Nodes=num_vertex,
Edges=num_edges,
Diameter=net_diameter,
Density=net_density,
Modularity=net_modularity,
Clustering.Coefficient=clustering_coefficient,
Betweeness=net_betweenness,
Closeness=net_closeness,
Connected.Components=connected_components,
Communities=length(labels_num$numeric)
)
# Save a general summary of all
datasets_summary <- read.csv(file=paste(datsets_location,'datasets_summary.csv',sep=''), sep = ',')
datasets_summary <- datasets_summary %>% filter(Name!=dataset_name)
datasets_summary <- datasets_summary %>% add_row(
Name=dataset_name,
Nodes=num_vertex,
Edges=num_edges,
Diameter=net_diameter,
Density=net_density,
Modularity=net_modularity,
Clustering.Coefficient=clustering_coefficient,
Betweeness=net_betweenness,
Closeness=net_closeness,
Connected.Components=connected_components,
Communities=length(as_data_frame(net, what='vertices')$V2)
)
write.csv(datasets_summary, file=paste(datsets_location,'datasets_summary.csv',sep=''), row.names=FALSE)
write.csv(datasets_summary, file=paste(datsets_location,'datasets_summary.csv',sep=''), row.names=FALSE)
datasets_summary <- datasets_summary %>% add_row(
Name=dataset_name,
Nodes=num_vertex,
Edges=num_edges,
Diameter=net_diameter,
Density=net_density,
Modularity=net_modularity,
Clustering.Coefficient=clustering_coefficient,
Betweeness=net_betweenness,
Closeness=net_closeness,
Connected.Components=connected_components,
Communities=length(unique(as_data_frame(net, what='vertices')$V2))
)
write.csv(datasets_summary, file=paste(datsets_location,'datasets_summary.csv',sep=''), row.names=FALSE)
write.csv(datasets_summary, file=paste(datsets_location,'datasets_summary.csv',sep=''), row.names=FALSE)
