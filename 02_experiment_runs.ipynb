{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "temporal-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "from igraph import Graph\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "import community as community_louvain\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from graph_modification import remove_edges, remove_vertices \n",
    "\n",
    "#https://igraph.org/python/doc/api/igraph.Graph.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "potential-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/palash1992/GEM\n",
    "\n",
    "from gem.evaluation import evaluate_graph_reconstruction as gr\n",
    "\n",
    "# Embedding imports\n",
    "from gem.embedding.gf       import GraphFactorization\n",
    "from gem.embedding.hope     import HOPE\n",
    "from gem.embedding.lap      import LaplacianEigenmaps\n",
    "from gem.embedding.lle      import LocallyLinearEmbedding\n",
    "from gem.embedding.node2vec import node2vec\n",
    "#from gem.embedding.sdne     import SDNE\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "filled-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading serialized datasets\n",
    "filename = 'datasets.data'\n",
    "\n",
    "infile = open(filename,'rb')\n",
    "datasets = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "specific-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline for every single one\n",
    "base_rows = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    p_size = float(dataset['vertices']['community'].nunique())\n",
    "\n",
    "    graph = dataset['graph']\n",
    "\n",
    "    # louvian\n",
    "    method = 'Louvain-igraph'\n",
    "\n",
    "    louvian = ig.Graph.community_multilevel(graph, return_levels=True)\n",
    "    louvian = louvian[len(louvian)-1]\n",
    "    p_louvian = len(set(louvian.membership))\n",
    "\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P',\n",
    "                 'value':p_louvian})    \n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P*/P',\n",
    "                 'value':p_size/p_louvian})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'Modularity',\n",
    "                 'value':graph.modularity(louvian.membership)})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'NMI',\n",
    "                 'value':normalized_mutual_info_score(dataset['vertices']['community'], louvian.membership)})\n",
    "\n",
    "    # FastGreedy\n",
    "    method = 'Fastgreedy'\n",
    "\n",
    "    fg = ig.Graph.community_fastgreedy(graph)\n",
    "    p_fg = fg.optimal_count\n",
    "    fg = fg.as_clustering()\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P',\n",
    "                 'value':p_fg})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P*/P',\n",
    "                 'value':p_size/p_fg})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'Modularity',\n",
    "                 'value':graph.modularity(fg.membership)})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'NMI',\n",
    "                 'value':normalized_mutual_info_score(dataset['vertices']['community'], fg.membership)})\n",
    "    # Infomap\n",
    "    method = 'Infomap'\n",
    "\n",
    "    infomap = ig.Graph.community_infomap(graph)\n",
    "    p_im = len(set(infomap.membership))\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P',\n",
    "                 'value':p_im})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P*/P',\n",
    "                 'value':p_size/p_im})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'Modularity',\n",
    "                 'value':graph.modularity(infomap.membership)})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'NMI',\n",
    "                 'value':normalized_mutual_info_score(dataset['vertices']['community'], infomap.membership)})\n",
    "\n",
    "    # Label Propagation\n",
    "    method = 'Label Propagation'\n",
    "\n",
    "    lp = ig.Graph.community_label_propagation(graph)\n",
    "    p_lp = len(set(lp.membership))\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P',\n",
    "                 'value':p_lp})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P*/P',\n",
    "                 'value':p_size/p_lp})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'Modularity',\n",
    "                 'value':graph.modularity(lp.membership)})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'NMI',\n",
    "                 'value':normalized_mutual_info_score(dataset['vertices']['community'], lp.membership)})\n",
    "\n",
    "    # Louvain\n",
    "    method = 'Louvain'\n",
    "\n",
    "    # Creating nx Graph to for other Louvain implementation\n",
    "    nxG = nx.Graph()\n",
    "    nxG.add_nodes_from([vertex.index for vertex in graph.vs])\n",
    "    nxG.add_edges_from([edge.tuple for edge in graph.es])\n",
    "\n",
    "    lv_partition = community_louvain.best_partition(nxG)\n",
    "    p_lv = len(set(lv_partition.values()))\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P',\n",
    "                 'value':p_lv})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P*/P',\n",
    "                 'value':p_size/p_lv})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'Modularity',\n",
    "                 'value':graph.modularity(lv_partition.values())})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'NMI',\n",
    "                 'value':normalized_mutual_info_score(dataset['vertices']['community'], pd.Series(lv_partition.values()))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values that will vary during the cycles \n",
    "\n",
    "pcts = [x*0.1 for x in range(1,10)]\n",
    "strategies = ['random','betweenness']\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Repeat the results for each of the experiments that are going to be run\n",
    "for base_row in base_rows:\n",
    "    for strat in strategies:\n",
    "        if strat == 'random':\n",
    "            orders = ['asc']\n",
    "        else:\n",
    "            orders = ['asc','desc']\n",
    "\n",
    "        for order in orders:\n",
    "            base_row['modification'] = strat+' '+order\n",
    "            base_row['percentage'] = 0.0\n",
    "\n",
    "            rows.append(base_row.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "basic-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for strat in strategies:\n",
    "        for pct in pcts:\n",
    "            if strat == 'random':\n",
    "                orders = ['asc']\n",
    "            else:\n",
    "                orders = ['asc','desc']\n",
    "\n",
    "            for order in orders:\n",
    "\n",
    "                p_size = float(dataset['vertices']['community'].nunique())\n",
    "\n",
    "                # modify the graph\n",
    "                if pct == 0:\n",
    "                    graph = dataset['graph']\n",
    "                else:\n",
    "                    graph = remove_edges(dataset['graph'], pct=pct, deep_copy=True, strategy=strat, order=order)\n",
    "\n",
    "                # louvian\n",
    "                method = 'Louvain-igraph'\n",
    "\n",
    "                louvian = ig.Graph.community_multilevel(graph, return_levels=True)\n",
    "                louvian = louvian[len(louvian)-1]\n",
    "                p_louvian = len(set(louvian.membership))\n",
    "\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P',\n",
    "                             'value':p_louvian})    \n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P*/P',\n",
    "                             'value':p_size/p_louvian})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'Modularity',\n",
    "                             'value':graph.modularity(louvian.membership)})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'NMI',\n",
    "                             'value':normalized_mutual_info_score(dataset['vertices']['community'], louvian.membership)})\n",
    "\n",
    "                # FastGreedy\n",
    "                method = 'Fastgreedy'\n",
    "\n",
    "                fg = ig.Graph.community_fastgreedy(graph)\n",
    "                p_fg = fg.optimal_count\n",
    "                fg = fg.as_clustering()\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P',\n",
    "                             'value':p_fg})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P*/P',\n",
    "                             'value':p_size/p_fg})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'Modularity',\n",
    "                             'value':graph.modularity(fg.membership)})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'NMI',\n",
    "                             'value':normalized_mutual_info_score(dataset['vertices']['community'], fg.membership)})\n",
    "                # Infomap\n",
    "                method = 'Infomap'\n",
    "\n",
    "                infomap = ig.Graph.community_infomap(graph)\n",
    "                p_im = len(set(infomap.membership))\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P',\n",
    "                             'value':p_im})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P*/P',\n",
    "                             'value':p_size/p_im})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'Modularity',\n",
    "                             'value':graph.modularity(infomap.membership)})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'NMI',\n",
    "                             'value':normalized_mutual_info_score(dataset['vertices']['community'], infomap.membership)})\n",
    "\n",
    "                # Label Propagation\n",
    "                method = 'Label Propagation'\n",
    "\n",
    "                lp = ig.Graph.community_label_propagation(graph)\n",
    "                p_lp = len(set(lp.membership))\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P',\n",
    "                             'value':p_lp})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P*/P',\n",
    "                             'value':p_size/p_lp})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'Modularity',\n",
    "                             'value':graph.modularity(lp.membership)})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'NMI',\n",
    "                             'value':normalized_mutual_info_score(dataset['vertices']['community'], lp.membership)})\n",
    "\n",
    "                # Louvain\n",
    "                method = 'Louvain'\n",
    "\n",
    "                # Creating nx Graph to for other Louvain implementation\n",
    "                nxG = nx.Graph()\n",
    "                nxG.add_nodes_from([vertex.index for vertex in graph.vs])\n",
    "                nxG.add_edges_from([edge.tuple for edge in graph.es])\n",
    "\n",
    "                lv_partition = community_louvain.best_partition(nxG)\n",
    "                p_lv = len(set(lv_partition.values()))\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P',\n",
    "                             'value':p_lv})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P*/P',\n",
    "                             'value':p_size/p_lv})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'Modularity',\n",
    "                             'value':graph.modularity(lv_partition.values())})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'NMI',\n",
    "                             'value':normalized_mutual_info_score(dataset['vertices']['community'], pd.Series(lv_partition.values()))})\n",
    "\n",
    "experiment_results = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/palash1992/GEM\n",
    "\n",
    "models = []\n",
    "# You can comment out the methods you don't want to run\n",
    "# GF takes embedding dimension (d), maximum iterations (max_iter), learning rate (eta), \n",
    "# regularization coefficient (regu) as inputs\n",
    "#models.append(GraphFactorization(d=2, max_iter=100000, eta=1*10**-4, regu=1.0, data_set='karate'))\n",
    "# HOPE takes embedding dimension (d) and decay factor (beta) as inputs\n",
    "models.append(HOPE(d=4, beta=0.01))\n",
    "# LE takes embedding dimension (d) as input\n",
    "models.append(LaplacianEigenmaps(d=2))\n",
    "# LLE takes embedding dimension (d) as input\n",
    "models.append(LocallyLinearEmbedding(d=2))\n",
    "# node2vec takes embedding dimension (d),  maximum iterations (max_iter), random walk length (walk_len), \n",
    "# number of random walks (num_walks), context size (con_size), return weight (ret_p), inout weight (inout_p) as inputs\n",
    "models.append(node2vec(d=2, max_iter=1, walk_len=80, num_walks=10, con_size=10, ret_p=1, inout_p=1))\n",
    "# SDNE takes embedding dimension (d), seen edge reconstruction weight (beta), first order proximity weight (alpha),\n",
    "# lasso regularization coefficient (nu1), ridge regreesion coefficient (nu2), number of hidden layers (K), \n",
    "# size of each layer (n_units), number of iterations (n_ite), learning rate (xeta), size of batch (n_batch),\n",
    "# location of modelfile and weightfile save (modelfile and weightfile) as inputs\n",
    "models.append(SDNE(d=2, beta=5, alpha=1e-5, nu1=1e-6, nu2=1e-6, K=3, n_units=[50, 15,], n_iter=50, xeta=0.01, n_batch=500,\n",
    "                modelfile=['enc_model.json', 'dec_model.json'],\n",
    "                weightfile=['enc_weights.hdf5', 'dec_weights.hdf5']))\n",
    "\n",
    "for embedding in models:\n",
    "    print ('Num nodes: %d, num edges: %d' % (G.number_of_nodes(), G.number_of_edges()))\n",
    "    t1 = time()\n",
    "    # Learn embedding - accepts a networkx graph or file with edge list\n",
    "    Y, t = embedding.learn_embedding(graph=G, edge_f=None, is_weighted=True, no_python=True)\n",
    "    print (embedding._method_name+':\\n\\tTraining time: %f' % (time() - t1))\n",
    "    # Evaluate on graph reconstruction\n",
    "    MAP, prec_curv, err, err_baseline = gr.evaluateStaticGraphReconstruction(G, embedding, Y, None)\n",
    "    #---------------------------------------------------------------------------------\n",
    "    print((\"\\tMAP: {} \\t precision curve: {}\\n\\n\\n\\n\"+'-'*100).format(MAP,prec_curv[:5]))\n",
    "    #---------------------------------------------------------------------------------\n",
    "    # Visualize\n",
    "    viz.plot_embedding2D(embedding.get_embedding(), di_graph=G, node_colors=None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "hungarian-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/palash1992/GEM\n",
    "\n",
    "models = []\n",
    "# You can comment out the methods you don't want to run\n",
    "# GF takes embedding dimension (d), maximum iterations (max_iter), learning rate (eta), \n",
    "# regularization coefficient (regu) as inputs\n",
    "#models.append(GraphFactorization(d=2, max_iter=100000, eta=1*10**-4, regu=1.0, data_set='karate'))\n",
    "# HOPE takes embedding dimension (d) and decay factor (beta) as inputs\n",
    "models.append(HOPE(d=4, beta=0.01))\n",
    "# LE takes embedding dimension (d) as input\n",
    "#models.append(LaplacianEigenmaps(d=2))\n",
    "# LLE takes embedding dimension (d) as input\n",
    "#models.append(LocallyLinearEmbedding(d=2))\n",
    "# node2vec takes embedding dimension (d),  maximum iterations (max_iter), random walk length (walk_len), \n",
    "# number of random walks (num_walks), context size (con_size), return weight (ret_p), inout weight (inout_p) as inputs\n",
    "#models.append(node2vec(d=2, max_iter=1, walk_len=80, num_walks=10, con_size=10, ret_p=1, inout_p=1))\n",
    "\n",
    "# This one throws compilation errors\n",
    "# SDNE takes embedding dimension (d), seen edge reconstruction weight (beta), first order proximity weight (alpha),\n",
    "# lasso regularization coefficient (nu1), ridge regreesion coefficient (nu2), number of hidden layers (K), \n",
    "# size of each layer (n_units), number of iterations (n_ite), learning rate (xeta), size of batch (n_batch),\n",
    "# location of modelfile and weightfile save (modelfile and weightfile) as inputs\n",
    "#models.append(SDNE(d=2, beta=5, alpha=1e-5, nu1=1e-6, nu2=1e-6, K=3, n_units=[50, 15,], n_iter=50, xeta=0.01, n_batch=500,\n",
    "#                modelfile=['enc_model.json', 'dec_model.json'],\n",
    "#                weightfile=['enc_weights.hdf5', 'dec_weights.hdf5']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "requested-chaos",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD error (low rank): 1.200452\n",
      "hope_gsvd:\n",
      "\tTraining time: 0.527995\n"
     ]
    }
   ],
   "source": [
    "other_rows = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for strat in strategies:\n",
    "        for pct in pcts:\n",
    "            if strat == 'random':\n",
    "                orders = ['asc']\n",
    "            else:\n",
    "                orders = ['asc','desc']\n",
    "\n",
    "            for order in orders:\n",
    "\n",
    "                p_size = float(dataset['vertices']['community'].nunique())\n",
    "\n",
    "                # modify the graph\n",
    "                if pct == 0:\n",
    "                    graph = dataset['graph']\n",
    "                else:\n",
    "                    #graph = remove_edges(dataset['graph'], pct=pct, deep_copy=True, strategy=strat, order=order)\n",
    "                    graph = dataset['graph']\n",
    "\n",
    "                # Creating nx Graph to for other Louvain implementation\n",
    "                nxG = nx.Graph()\n",
    "                nxG.add_nodes_from([vertex.index for vertex in graph.vs])\n",
    "                nxG.add_edges_from([edge.tuple for edge in graph.es])\n",
    "                \n",
    "                for embedding in models:\n",
    "                    \n",
    "                    method = 'embedding '+embedding.get_method_name()\n",
    "                    \n",
    "                    t1 = time()\n",
    "                    # Learn embedding - accepts a networkx graph or file with edge list\n",
    "                    Y, t = embedding.learn_embedding(graph=nxG, edge_f=None, is_weighted=True, no_python=True)\n",
    "                    t2 = time()\n",
    "                    print (embedding._method_name+':\\n\\tTraining time: %f' % (time() - t1))\n",
    "                    \n",
    "                    kmeans = KMeans(n_clusters=int(p_size), random_state=1234).fit(embedding.get_embedding())\n",
    "                    kmeans.labels_\n",
    "\n",
    "                    other_rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'Modularity',\n",
    "                                 'value':graph.modularity(kmeans.labels_)})\n",
    "                    other_rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'NMI',\n",
    "                                 'value':normalized_mutual_info_score(dataset['vertices']['community'], pd.Series(kmeans.labels_))})\n",
    "\n",
    "\n",
    "                break\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "blond-district",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dataset': 'Dancer 01',\n",
       "  'modification': 'random asc',\n",
       "  'percentage': 0.0,\n",
       "  'method': 'embedding hope_gsvd',\n",
       "  'metric': 'Modularity',\n",
       "  'value': 0.18246776733992995},\n",
       " {'dataset': 'Dancer 01',\n",
       "  'modification': 'random asc',\n",
       "  'percentage': 0.0,\n",
       "  'method': 'embedding hope_gsvd',\n",
       "  'metric': 'NMI',\n",
       "  'value': 0.3286363859268051}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "restricted-lindsay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.10630912e-02,  1.15072035e-02,  7.10630912e-02,\n",
       "         1.15072035e-02],\n",
       "       [ 6.10248805e-04,  5.80173837e-03,  6.10248805e-04,\n",
       "         5.80173837e-03],\n",
       "       [-1.36572664e-02,  7.51903366e-02, -1.36572664e-02,\n",
       "         7.51903366e-02],\n",
       "       ...,\n",
       "       [-5.03911955e-04,  2.94618728e-03, -5.03911955e-04,\n",
       "         2.94618728e-03],\n",
       "       [-2.42047322e-05,  1.24574650e-03, -2.42047322e-05,\n",
       "         1.24574650e-03],\n",
       "       [-2.37549097e-04,  1.18389495e-03, -2.37549097e-04,\n",
       "         1.18389495e-03]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "available-intent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>method</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>modification</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dancer 01</td>\n",
       "      <td>Louvain-igraph</td>\n",
       "      <td>P</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>betweenness desc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dancer 01</td>\n",
       "      <td>Louvain-igraph</td>\n",
       "      <td>P</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>betweenness desc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dancer 01</td>\n",
       "      <td>Louvain-igraph</td>\n",
       "      <td>P</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>betweenness desc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dancer 01</td>\n",
       "      <td>Louvain-igraph</td>\n",
       "      <td>P*/P</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>betweenness desc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dancer 01</td>\n",
       "      <td>Louvain-igraph</td>\n",
       "      <td>P*/P</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>betweenness desc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset          method metric     value      modification  percentage\n",
       "0  Dancer 01  Louvain-igraph      P  9.000000  betweenness desc         0.0\n",
       "1  Dancer 01  Louvain-igraph      P  9.000000  betweenness desc         0.0\n",
       "2  Dancer 01  Louvain-igraph      P  9.000000  betweenness desc         0.0\n",
       "3  Dancer 01  Louvain-igraph   P*/P  0.666667  betweenness desc         0.0\n",
       "4  Dancer 01  Louvain-igraph   P*/P  0.666667  betweenness desc         0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "preliminary-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.to_csv('experiment_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
