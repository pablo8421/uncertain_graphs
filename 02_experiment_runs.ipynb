{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "temporal-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "from igraph import Graph\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "import community as community_louvain\n",
    "import networkx as nx\n",
    "\n",
    "from graph_modification import remove_edges, remove_vertices \n",
    "\n",
    "#https://igraph.org/python/doc/api/igraph.Graph.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding imports\n",
    "from gem.embedding.gf       import GraphFactorization\n",
    "from gem.embedding.hope     import HOPE\n",
    "from gem.embedding.lap      import LaplacianEigenmaps\n",
    "from gem.embedding.lle      import LocallyLinearEmbedding\n",
    "from gem.embedding.node2vec import node2vec\n",
    "from gem.embedding.sdne     import SDNE\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "filled-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading serialized datasets\n",
    "filename = 'datasets.data'\n",
    "\n",
    "infile = open(filename,'rb')\n",
    "datasets = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "endangered-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline for every single one\n",
    "base_rows = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    p_size = float(dataset['vertices']['community'].nunique())\n",
    "\n",
    "    graph = dataset['graph']\n",
    "\n",
    "    # louvian\n",
    "    method = 'Louvain-igraph'\n",
    "\n",
    "    louvian = ig.Graph.community_multilevel(graph, return_levels=True)\n",
    "    louvian = louvian[len(louvian)-1]\n",
    "    p_louvian = len(set(louvian.membership))\n",
    "\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P',\n",
    "                 'value':p_louvian})    \n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P*/P',\n",
    "                 'value':p_size/p_louvian})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'Modularity',\n",
    "                 'value':graph.modularity(louvian.membership)})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'NMI',\n",
    "                 'value':normalized_mutual_info_score(dataset['vertices']['community'], louvian.membership)})\n",
    "\n",
    "    # FastGreedy\n",
    "    method = 'Fastgreedy'\n",
    "\n",
    "    fg = ig.Graph.community_fastgreedy(graph)\n",
    "    p_fg = fg.optimal_count\n",
    "    fg = fg.as_clustering()\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P',\n",
    "                 'value':p_fg})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P*/P',\n",
    "                 'value':p_size/p_fg})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'Modularity',\n",
    "                 'value':graph.modularity(fg.membership)})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'NMI',\n",
    "                 'value':normalized_mutual_info_score(dataset['vertices']['community'], fg.membership)})\n",
    "    # Infomap\n",
    "    method = 'Infomap'\n",
    "\n",
    "    infomap = ig.Graph.community_infomap(graph)\n",
    "    p_im = len(set(infomap.membership))\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P',\n",
    "                 'value':p_im})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P*/P',\n",
    "                 'value':p_size/p_im})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'Modularity',\n",
    "                 'value':graph.modularity(infomap.membership)})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'NMI',\n",
    "                 'value':normalized_mutual_info_score(dataset['vertices']['community'], infomap.membership)})\n",
    "\n",
    "    # Label Propagation\n",
    "    method = 'Label Propagation'\n",
    "\n",
    "    lp = ig.Graph.community_label_propagation(graph)\n",
    "    p_lp = len(set(lp.membership))\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P',\n",
    "                 'value':p_lp})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P*/P',\n",
    "                 'value':p_size/p_lp})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'Modularity',\n",
    "                 'value':graph.modularity(lp.membership)})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'NMI',\n",
    "                 'value':normalized_mutual_info_score(dataset['vertices']['community'], lp.membership)})\n",
    "\n",
    "    # Louvain\n",
    "    method = 'Louvain'\n",
    "\n",
    "    # Creating nx Graph to for other Louvain implementation\n",
    "    nxG = nx.Graph()\n",
    "    nxG.add_nodes_from([vertex.index for vertex in graph.vs])\n",
    "    nxG.add_edges_from([edge.tuple for edge in graph.es])\n",
    "\n",
    "    lv_partition = community_louvain.best_partition(nxG)\n",
    "    p_lv = len(set(lv_partition.values()))\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P',\n",
    "                 'value':p_lv})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'P*/P',\n",
    "                 'value':p_size/p_lv})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'Modularity',\n",
    "                 'value':graph.modularity(lv_partition.values())})\n",
    "    base_rows.append({'dataset':dataset['name'], 'method':method, 'metric':'NMI',\n",
    "                 'value':normalized_mutual_info_score(dataset['vertices']['community'], pd.Series(lv_partition.values()))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values that will vary during the cycles \n",
    "\n",
    "pcts = [x*0.1 for x in range(1,10)]\n",
    "strategies = ['random','betweenness']\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Repeat the results for each of the experiments that are going to be run\n",
    "for base_row in base_rows:\n",
    "    for strat in strategies:\n",
    "        if strat == 'random':\n",
    "            orders = ['asc']\n",
    "        else:\n",
    "            orders = ['asc','desc']\n",
    "\n",
    "        for order in orders:\n",
    "            base_row['modification'] = strat+' '+order\n",
    "            base_row['percentage'] = 0.0\n",
    "\n",
    "            rows.append(base_row.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "basic-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for strat in strategies:\n",
    "        for pct in pcts:\n",
    "            if strat == 'random':\n",
    "                orders = ['asc']\n",
    "            else:\n",
    "                orders = ['asc','desc']\n",
    "\n",
    "            for order in orders:\n",
    "\n",
    "                p_size = float(dataset['vertices']['community'].nunique())\n",
    "\n",
    "                # modify the graph\n",
    "                if pct == 0:\n",
    "                    graph = dataset['graph']\n",
    "                else:\n",
    "                    graph = remove_edges(dataset['graph'], pct=pct, deep_copy=True, strategy=strat, order=order)\n",
    "\n",
    "                # louvian\n",
    "                method = 'Louvain-igraph'\n",
    "\n",
    "                louvian = ig.Graph.community_multilevel(graph, return_levels=True)\n",
    "                louvian = louvian[len(louvian)-1]\n",
    "                p_louvian = len(set(louvian.membership))\n",
    "\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P',\n",
    "                             'value':p_louvian})    \n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P*/P',\n",
    "                             'value':p_size/p_louvian})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'Modularity',\n",
    "                             'value':graph.modularity(louvian.membership)})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'NMI',\n",
    "                             'value':normalized_mutual_info_score(dataset['vertices']['community'], louvian.membership)})\n",
    "\n",
    "                # FastGreedy\n",
    "                method = 'Fastgreedy'\n",
    "\n",
    "                fg = ig.Graph.community_fastgreedy(graph)\n",
    "                p_fg = fg.optimal_count\n",
    "                fg = fg.as_clustering()\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P',\n",
    "                             'value':p_fg})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P*/P',\n",
    "                             'value':p_size/p_fg})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'Modularity',\n",
    "                             'value':graph.modularity(fg.membership)})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'NMI',\n",
    "                             'value':normalized_mutual_info_score(dataset['vertices']['community'], fg.membership)})\n",
    "                # Infomap\n",
    "                method = 'Infomap'\n",
    "\n",
    "                infomap = ig.Graph.community_infomap(graph)\n",
    "                p_im = len(set(infomap.membership))\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P',\n",
    "                             'value':p_im})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P*/P',\n",
    "                             'value':p_size/p_im})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'Modularity',\n",
    "                             'value':graph.modularity(infomap.membership)})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'NMI',\n",
    "                             'value':normalized_mutual_info_score(dataset['vertices']['community'], infomap.membership)})\n",
    "\n",
    "                # Label Propagation\n",
    "                method = 'Label Propagation'\n",
    "\n",
    "                lp = ig.Graph.community_label_propagation(graph)\n",
    "                p_lp = len(set(lp.membership))\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P',\n",
    "                             'value':p_lp})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P*/P',\n",
    "                             'value':p_size/p_lp})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'Modularity',\n",
    "                             'value':graph.modularity(lp.membership)})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'NMI',\n",
    "                             'value':normalized_mutual_info_score(dataset['vertices']['community'], lp.membership)})\n",
    "\n",
    "                # Louvain\n",
    "                method = 'Louvain'\n",
    "\n",
    "                # Creating nx Graph to for other Louvain implementation\n",
    "                nxG = nx.Graph()\n",
    "                nxG.add_nodes_from([vertex.index for vertex in graph.vs])\n",
    "                nxG.add_edges_from([edge.tuple for edge in graph.es])\n",
    "\n",
    "                lv_partition = community_louvain.best_partition(nxG)\n",
    "                p_lv = len(set(lv_partition.values()))\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P',\n",
    "                             'value':p_lv})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'P*/P',\n",
    "                             'value':p_size/p_lv})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'Modularity',\n",
    "                             'value':graph.modularity(lv_partition.values())})\n",
    "                rows.append({'dataset':dataset['name'], 'modification':strat+' '+order, 'percentage':pct, 'method':method, 'metric':'NMI',\n",
    "                             'value':normalized_mutual_info_score(dataset['vertices']['community'], pd.Series(lv_partition.values()))})\n",
    "\n",
    "experiment_results = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/palash1992/GEM\n",
    "\n",
    "models = []\n",
    "# You can comment out the methods you don't want to run\n",
    "# GF takes embedding dimension (d), maximum iterations (max_iter), learning rate (eta), \n",
    "# regularization coefficient (regu) as inputs\n",
    "#models.append(GraphFactorization(d=2, max_iter=100000, eta=1*10**-4, regu=1.0, data_set='karate'))\n",
    "# HOPE takes embedding dimension (d) and decay factor (beta) as inputs\n",
    "models.append(HOPE(d=4, beta=0.01))\n",
    "# LE takes embedding dimension (d) as input\n",
    "models.append(LaplacianEigenmaps(d=2))\n",
    "# LLE takes embedding dimension (d) as input\n",
    "models.append(LocallyLinearEmbedding(d=2))\n",
    "# node2vec takes embedding dimension (d),  maximum iterations (max_iter), random walk length (walk_len), \n",
    "# number of random walks (num_walks), context size (con_size), return weight (ret_p), inout weight (inout_p) as inputs\n",
    "models.append(node2vec(d=2, max_iter=1, walk_len=80, num_walks=10, con_size=10, ret_p=1, inout_p=1))\n",
    "# SDNE takes embedding dimension (d), seen edge reconstruction weight (beta), first order proximity weight (alpha),\n",
    "# lasso regularization coefficient (nu1), ridge regreesion coefficient (nu2), number of hidden layers (K), \n",
    "# size of each layer (n_units), number of iterations (n_ite), learning rate (xeta), size of batch (n_batch),\n",
    "# location of modelfile and weightfile save (modelfile and weightfile) as inputs\n",
    "models.append(SDNE(d=2, beta=5, alpha=1e-5, nu1=1e-6, nu2=1e-6, K=3, n_units=[50, 15,], n_iter=50, xeta=0.01, n_batch=500,\n",
    "                modelfile=['enc_model.json', 'dec_model.json'],\n",
    "                weightfile=['enc_weights.hdf5', 'dec_weights.hdf5']))\n",
    "\n",
    "for embedding in models:\n",
    "    print ('Num nodes: %d, num edges: %d' % (G.number_of_nodes(), G.number_of_edges()))\n",
    "    t1 = time()\n",
    "    # Learn embedding - accepts a networkx graph or file with edge list\n",
    "    Y, t = embedding.learn_embedding(graph=G, edge_f=None, is_weighted=True, no_python=True)\n",
    "    print (embedding._method_name+':\\n\\tTraining time: %f' % (time() - t1))\n",
    "    # Evaluate on graph reconstruction\n",
    "    MAP, prec_curv, err, err_baseline = gr.evaluateStaticGraphReconstruction(G, embedding, Y, None)\n",
    "    #---------------------------------------------------------------------------------\n",
    "    print((\"\\tMAP: {} \\t precision curve: {}\\n\\n\\n\\n\"+'-'*100).format(MAP,prec_curv[:5]))\n",
    "    #---------------------------------------------------------------------------------\n",
    "    # Visualize\n",
    "    viz.plot_embedding2D(embedding.get_embedding(), di_graph=G, node_colors=None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/palash1992/GEM\n",
    "\n",
    "models = []\n",
    "# You can comment out the methods you don't want to run\n",
    "# GF takes embedding dimension (d), maximum iterations (max_iter), learning rate (eta), \n",
    "# regularization coefficient (regu) as inputs\n",
    "#models.append(GraphFactorization(d=2, max_iter=100000, eta=1*10**-4, regu=1.0, data_set='karate'))\n",
    "# HOPE takes embedding dimension (d) and decay factor (beta) as inputs\n",
    "models.append(HOPE(d=4, beta=0.01))\n",
    "# LE takes embedding dimension (d) as input\n",
    "models.append(LaplacianEigenmaps(d=2))\n",
    "# LLE takes embedding dimension (d) as input\n",
    "models.append(LocallyLinearEmbedding(d=2))\n",
    "# node2vec takes embedding dimension (d),  maximum iterations (max_iter), random walk length (walk_len), \n",
    "# number of random walks (num_walks), context size (con_size), return weight (ret_p), inout weight (inout_p) as inputs\n",
    "models.append(node2vec(d=2, max_iter=1, walk_len=80, num_walks=10, con_size=10, ret_p=1, inout_p=1))\n",
    "# SDNE takes embedding dimension (d), seen edge reconstruction weight (beta), first order proximity weight (alpha),\n",
    "# lasso regularization coefficient (nu1), ridge regreesion coefficient (nu2), number of hidden layers (K), \n",
    "# size of each layer (n_units), number of iterations (n_ite), learning rate (xeta), size of batch (n_batch),\n",
    "# location of modelfile and weightfile save (modelfile and weightfile) as inputs\n",
    "models.append(SDNE(d=2, beta=5, alpha=1e-5, nu1=1e-6, nu2=1e-6, K=3, n_units=[50, 15,], n_iter=50, xeta=0.01, n_batch=500,\n",
    "                modelfile=['enc_model.json', 'dec_model.json'],\n",
    "                weightfile=['enc_weights.hdf5', 'dec_weights.hdf5']))\n",
    "\n",
    "for embedding in models:\n",
    "    print ('Num nodes: %d, num edges: %d' % (G.number_of_nodes(), G.number_of_edges()))\n",
    "    t1 = time()\n",
    "    # Learn embedding - accepts a networkx graph or file with edge list\n",
    "    Y, t = embedding.learn_embedding(graph=G, edge_f=None, is_weighted=True, no_python=True)\n",
    "    print (embedding._method_name+':\\n\\tTraining time: %f' % (time() - t1))\n",
    "    # Evaluate on graph reconstruction\n",
    "    MAP, prec_curv, err, err_baseline = gr.evaluateStaticGraphReconstruction(G, embedding, Y, None)\n",
    "    #---------------------------------------------------------------------------------\n",
    "    print((\"\\tMAP: {} \\t precision curve: {}\\n\\n\\n\\n\"+'-'*100).format(MAP,prec_curv[:5]))\n",
    "    #---------------------------------------------------------------------------------\n",
    "    # Visualize\n",
    "    viz.plot_embedding2D(embedding.get_embedding(), di_graph=G, node_colors=None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "square-trouble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    for strat in strategies:\n",
    "        for pct in pcts:\n",
    "            if strat == 'random':\n",
    "                orders = ['asc']\n",
    "            else:\n",
    "                orders = ['asc','desc']\n",
    "\n",
    "            for order in orders:\n",
    "\n",
    "                p_size = float(dataset['vertices']['community'].nunique())\n",
    "\n",
    "                # modify the graph\n",
    "                if pct == 0:\n",
    "                    graph = dataset['graph']\n",
    "                else:\n",
    "                    #graph = remove_edges(dataset['graph'], pct=pct, deep_copy=True, strategy=strat, order=order)\n",
    "                    graph = dataset['graph']\n",
    "\n",
    "                # Creating nx Graph to for other Louvain implementation\n",
    "                nxG = nx.Graph()\n",
    "                nxG.add_nodes_from([vertex.index for vertex in graph.vs])\n",
    "                nxG.add_edges_from([edge.tuple for edge in graph.es])\n",
    "                \n",
    "                print('Hola')\n",
    "                break\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "available-intent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>method</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>modification</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dancer 01</td>\n",
       "      <td>Louvain-igraph</td>\n",
       "      <td>P</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>betweenness desc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dancer 01</td>\n",
       "      <td>Louvain-igraph</td>\n",
       "      <td>P</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>betweenness desc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dancer 01</td>\n",
       "      <td>Louvain-igraph</td>\n",
       "      <td>P</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>betweenness desc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dancer 01</td>\n",
       "      <td>Louvain-igraph</td>\n",
       "      <td>P*/P</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>betweenness desc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dancer 01</td>\n",
       "      <td>Louvain-igraph</td>\n",
       "      <td>P*/P</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>betweenness desc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset          method metric     value      modification  percentage\n",
       "0  Dancer 01  Louvain-igraph      P  9.000000  betweenness desc         0.0\n",
       "1  Dancer 01  Louvain-igraph      P  9.000000  betweenness desc         0.0\n",
       "2  Dancer 01  Louvain-igraph      P  9.000000  betweenness desc         0.0\n",
       "3  Dancer 01  Louvain-igraph   P*/P  0.666667  betweenness desc         0.0\n",
       "4  Dancer 01  Louvain-igraph   P*/P  0.666667  betweenness desc         0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "preliminary-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results.to_csv('experiment_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
